/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
package com.mozilla.telemetry.utils

import java.io.ByteArrayInputStream
import java.time.format.DateTimeFormatter
import java.time.format.DateTimeFormatter.{ISO_DATE, ISO_DATE_TIME}
import java.time.{LocalDate, ZonedDateTime}
import java.util.zip.GZIPInputStream

import com.mozilla.telemetry.heka.Dataset
import org.apache.commons.io.IOUtils.toByteArray
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, decode, lit, udf}
import org.json4s.JValue
import org.json4s.JsonAST.{JInt, JObject, JString}
import org.json4s.jackson.JsonMethods.parse

class DatasetShim private(submissionDate: String, documentNamespace: String, documentType: String, documentVersion: String, normalizedAppName: String,
                          normalizedChannel: Option[String], appVersion: Option[String]) {
  private val logger = org.apache.log4j.Logger.getLogger(this.getClass.getName)

  def fromHeka(limit: Option[Int] = None, numPartitions: Option[Int] = None)(implicit sc: SparkContext): RDD[Option[JValue]] = {
    Map(
      "submissionDate" -> Some(submissionDate),
      "sourceName" -> Some(documentNamespace),
      "sourceVersion" -> Some(documentVersion),
      "appName" -> Some(normalizedAppName),
      "appUpdateChannel" -> normalizedChannel,
      "appVersion" -> appVersion)
      .collect {case (key, Some(value)) => key -> value}
      .foldLeft(Dataset(submissionDate match {
        case d if d < "20161012" => "telemetry-oldinfra"
        case _ => "telemetry"
      })){case (ds, (dimension, expect)) => ds.where(dimension){case actual => actual == expect}}
      .records(limit, numPartitions).map(_.toJValue)
  }

  def fromBigQuery(limit: Option[Int] = None, parallelism: Option[Int] = None,
                   dataset: Option[String] = None)(implicit spark: SparkSession): RDD[Option[JValue]] = {
    // add dashes to submission_date for backwards compatibility
    val isoSubmissionDate = LocalDate.parse(submissionDate, DatasetShim.DATE_NO_DASH).format(ISO_DATE)

    val actualParallelism = parallelism.getOrElse(spark.sparkContext.defaultParallelism)
    logger.info(s"Reading from BigQuery with parallelism: $actualParallelism")
    val messages = spark.read
      .format("bigquery")
      .option("dataset", dataset.getOrElse("payload_bytes_decoded"))
      .option("parallelism", actualParallelism)
      .option("table", s"telemetry_${documentNamespace}__${documentType}_v$documentVersion")
      .option("filter", s"CAST(submission_timestamp AS DATE) = '$isoSubmissionDate'")
      .load()

    val limited = limit match {
      case Some(x) => messages.limit(x)
      case _ => messages
    }

    // udf for decompressing payload
    val gunzip = udf((value: Array[Byte]) => toByteArray(new GZIPInputStream(new ByteArrayInputStream(value))))

    Map(
      "normalized_app_name" -> Some(normalizedAppName),
      "normalized_channel" -> normalizedChannel,
      "metadata.uri.app_version" -> appVersion)
      .collect {case (key, Some(value)) => key -> value}
      .foldLeft(limited) {case (df, (attribute, expect)) => df.where(col(attribute).equalTo(lit(expect)))}
      .select("payload")
      .select(decode(gunzip(col("payload")), "UTF-8"))
      .rdd.map(row => parse(row.getString(0)))
      .map(doc => {
        // submission_timestamp is generated by the edge server and must be 'an ISO 8601 timestamp with microseconds and timezone "Z"'
        // https://github.com/mozilla/gcp-ingestion/blob/master/docs/edge.md#edge-server-pubsub-message-schema
        val JString(st) = doc \ "submission_timestamp"
        val submissionTimestamp = ZonedDateTime.parse(st, ISO_DATE_TIME)
        // provide doc \ "meta" to better match com.mozilla.telemetry.heka.Message.toJValue
        Some(doc ++ JObject(List(
          ("meta", JObject(List(
            ("submissionDate", JString(submissionTimestamp.format(DatasetShim.DATE_NO_DASH))),
            ("Timestamp", JInt(
              submissionTimestamp.toEpochSecond * 1e9.toLong + submissionTimestamp.getNano.toLong
            )),
            ("documentId", doc \ "document_id"),
            ("clientId", doc \ "clientId"),
            ("sampleId", doc \ "sample_id"),
            ("appUpdateChannel", doc \ "metadata" \ "uri" \ "app_update_channel"),
            ("normalizedChannel", doc \ "normalized_channel"),
            ("normalizedOSVersion", doc \ "normalized_os_version"),
            ("Date", doc \ "metadata" \ "header" \ "date"),
            ("geoCountry", doc \ "metadata" \ "geo" \ "country"),
            ("geoCity", doc \ "metadata" \ "geo" \ "city"),
            ("geoSubdivision1", doc \ "metadata" \ "geo" \ "subdivision1"),
            ("geoSubdivision2", doc \ "metadata" \ "geo" \ "subdivision2")
          )))
        )))
      })
  }
}

object DatasetShim {
  final val DATE_NO_DASH: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyyMMdd")

  def apply(submissionDate: String, documentNamespace: String = "telemetry", documentType: String = "main", documentVersion: String = "4",
            normalizedAppName: String = "Firefox", normalizedChannel: Option[String] = None, appVersion: Option[String] = None): DatasetShim =
    new DatasetShim(submissionDate, documentNamespace, documentType, documentVersion, normalizedAppName, normalizedChannel, appVersion)
}
